{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc263e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.sac.policies import MlpPolicy\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3 import SAC\n",
    "import logging\n",
    "import gym\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# add reference libraries here. Current structure will use the relative path from this file\n",
    "libs = [\"../../OpenIPSL-1.5.0/OpenIPSL/package.mo\",\n",
    "        \"../resources/KundurSMIB/package.mo\"] # KundurSMIB modified to have voltage control\n",
    "\n",
    "# check that all the paths to library package.mo files exist\n",
    "# DymolaInterface() also checks this but I've found this warning helpful\n",
    "for lib in libs:\n",
    "    if not os.path.isfile(lib):\n",
    "        print(f\"Cannot find the library {lib}\")\n",
    "\n",
    "mo_name = \"KundurSMIB.SMIB_vref\" # name of Modelica model in the Library.Model format\n",
    "env_entry_point = 'examples:DymSMIBEnv' # Python package location of RL environment\n",
    "\n",
    "v_ref = 1\n",
    "time_step = 1 # time delta in seconds\n",
    "positive_reward = 1\n",
    "negative_reward = -100 # penalize RL agent for is_done\n",
    "log_level = logging.DEBUG\n",
    "\n",
    "# these config values are passed to the model specific environment class\n",
    "# mo_name and libs are passed on to the DymolaBaseEnv class\n",
    "config = {\n",
    "    'mo_name': mo_name,\n",
    "    'libs': libs,\n",
    "    'v_ref': v_ref,\n",
    "    'time_step': time_step,\n",
    "    'positive_reward': positive_reward,\n",
    "    'negative_reward': negative_reward,\n",
    "    'log_level': log_level\n",
    "}\n",
    "\n",
    "# enable the model specific class as an OpenAI gym environment\n",
    "from gym.envs.registration import register\n",
    "env_name = \"MicrogridEnv-v0\"\n",
    "\n",
    "register(\n",
    "    id=env_name,\n",
    "    entry_point=env_entry_point,\n",
    "    kwargs=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a70b103e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[True, [0.9415191647391123]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aisling\\.conda\\envs\\myenv\\lib\\site-packages\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "# create the environment. this will run an initial step and must return [True, [...]] or something is broken\n",
    "# TODO: create error handling/warnings if simulations don't work (i.e. returns [False], [...])\n",
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ba0ee00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[True, [0.9415191647391123]]\n",
      "[True, [0.7838888263120665]]\n",
      "[True, [0.5778502347409977]]\n",
      "[True, [0.37689214331774173]]\n",
      "[True, [-0.4102310273435199]]\n",
      "[True, [-0.39768415229741216]]\n",
      "[True, [0.26086190725956715]]\n",
      "[True, [0.2306681763146432]]\n",
      "[True, [-0.3630533068700842]]\n",
      "[True, [-0.20653164435361318]]\n",
      "[True, [-0.3770097767658315]]\n"
     ]
    }
   ],
   "source": [
    "# this bit is for normalizing the reward later (to improve training), can be safely ignored for now\n",
    "min_reward = np.inf\n",
    "max_reward = -np.inf\n",
    "avg_reward = 0\n",
    "obs = env.reset()\n",
    "\n",
    "# show performance over 10 seconds in a do-nothing case (control voltage set at 1.0 pu)\n",
    "for _ in range(10):\n",
    "    action = [1.0] # control voltage = 1.0 pu\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    if done:\n",
    "        env.reset()\n",
    "    \n",
    "    # a continuation of the reward normalizing piece (can be ignored for now)\n",
    "    avg_reward += 1/30 * reward\n",
    "    if reward < min_reward:\n",
    "        min_reward = reward\n",
    "    if reward > max_reward:\n",
    "        max_reward = reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56fcc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset environment\n",
    "obs = env.reset()\n",
    "\n",
    "# run a randomized agent to verify:\n",
    "#    (1) that the simulation runs when we are controlling and changing an input value\n",
    "#    (2) that the simulation outputs different results than the do-nothing or rule-based controller\n",
    "for _ in range(10):\n",
    "    action = [np.random.uniform(1.0,2.0)]\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    if done:\n",
    "        env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964d4ec6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create a stable-baselines Soft Actor Critic agent\n",
    "model = SAC(MlpPolicy, env, verbose=1, tensorboard_log=\"tensorboard_logs\")\n",
    "\n",
    "# run a short training period to verify that the syntax is ok\n",
    "print(\"Training the model...\")\n",
    "obs = env.reset()\n",
    "model.learn(total_timesteps=20, tb_log_name=\"microgrid\")\n",
    "\n",
    "# run a short test period to verify that the syntax is ok\n",
    "print(\"Testing the model...\")\n",
    "obs = env.reset()\n",
    "rl_reward = 0\n",
    "for _ in range(10):\n",
    "    action, _state = model.predict(obs)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    if done:\n",
    "        env.reset()\n",
    "    rl_reward += reward"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-myenv] *",
   "language": "python",
   "name": "conda-env-.conda-myenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
