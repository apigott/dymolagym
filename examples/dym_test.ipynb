{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc263e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.sac.policies import MlpPolicy\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3 import SAC\n",
    "import logging\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "libs = [\"../../OpenIPSL-1.5.0/OpenIPSL/package.mo\",\n",
    "        \"../../OpenIPSL-1.5.0/ApplicationExamples/KundurSMIB/package.mo\"]\n",
    "mo_name = \"KundurSMIB.SMIB_vref\"\n",
    "env_entry_point = 'examples:DymSMIBEnv'\n",
    "\n",
    "v_ref = 1\n",
    "time_step = 1\n",
    "positive_reward = 1\n",
    "negative_reward = -100\n",
    "log_level = logging.DEBUG\n",
    "\n",
    "config = {\n",
    "    'mo_name': mo_name,\n",
    "    'libs': libs,\n",
    "    'v_ref': v_ref,\n",
    "    'time_step': time_step,\n",
    "    'positive_reward': positive_reward,\n",
    "    'negative_reward': negative_reward,\n",
    "    'log_level': log_level\n",
    "}\n",
    "\n",
    "from gym.envs.registration import register\n",
    "env_name = \"MicrogridEnv-v0\"\n",
    "\n",
    "register(\n",
    "    id=env_name,\n",
    "    entry_point=env_entry_point,\n",
    "    kwargs=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a70b103e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[True, [0.0004939890731558813]]\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aisling\\.conda\\envs\\myenv\\lib\\site-packages\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(env_name)\n",
    "# env.dymola.cd(os.path.join(os.getcwd())\n",
    "model = SAC(MlpPolicy, env, verbose=1, tensorboard_log=\"tensorboard_logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f38848e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ba0ee00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:List of values for model inputs should be of the length 0,equal to the number of model inputs. Actual length 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[True, [0.0004939890731558813]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "List of values for model inputs should be of the length 0,equal to the number of model inputs. Actual length 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-ed057fc40376>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mmy_action\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.05\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmy_action\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\aisling\\documents\\modelicagym\\examples\\dym_smib_env.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_steps\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'human'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\aisling\\documents\\modelicagym\\modelicagym\\environment\\dymola_base_env.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    127\u001b[0m                 len(list(self.model_input_names)), len(action))\n\u001b[0;32m    128\u001b[0m             \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 129\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    130\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m         \u001b[1;31m# Set input values of the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: List of values for model inputs should be of the length 0,equal to the number of model inputs. Actual length 1"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "min_reward = np.inf\n",
    "max_reward = -np.inf\n",
    "avg_reward = 0\n",
    "obs = env.reset()\n",
    "for _ in range(20):\n",
    "    action = [1.0]\n",
    "    my_action = np.multiply(0.05,action) + 1\n",
    "    obs, reward, done, info = env.step(my_action)\n",
    "    if done:\n",
    "        env.reset()\n",
    "    avg_reward += 1/30 * reward\n",
    "    if reward < min_reward:\n",
    "        min_reward = reward\n",
    "    if reward > max_reward:\n",
    "        max_reward = reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964d4ec6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = SAC(MlpPolicy, env, verbose=1, tensorboard_log=\"tensorboard_logs\")\n",
    "\n",
    "obs = env.reset()\n",
    "rl_reward = 0\n",
    "for _ in range(20):\n",
    "    action, _state = model.predict(obs)\n",
    "    my_action = np.multiply(0.05,action) + 1\n",
    "    obs, reward, done, info = env.step(my_action)\n",
    "    if done:\n",
    "        env.reset()\n",
    "    rl_reward += reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec7fa66",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.dymola.getLastErrorLog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f39c0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.dymola.simulateExtendedModel(env.model_name, startTime=0,\n",
    "                                                stopTime=1,\n",
    "                                                 initialNames=['v_ref'],\n",
    "                                                 initialValues=[1.0],\n",
    "                                                finalNames=env.model_output_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000dd50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c4637c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fddb67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-myenv] *",
   "language": "python",
   "name": "conda-env-.conda-myenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
