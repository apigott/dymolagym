{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc263e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.sac.policies import MlpPolicy\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3 import SAC\n",
    "import logging\n",
    "import gym\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# add reference libraries here. Current structure will use the relative path from this file\n",
    "libs = [\"../../OpenIPSL-1.5.0/OpenIPSL/package.mo\",\n",
    "        \"../../OpenIPSL-1.5.0/ApplicationExamples/KundurSMIB/package.mo\"] # KundurSMIB modified to have voltage control\n",
    "\n",
    "# check that all the paths to library package.mo files exist\n",
    "# DymolaInterface() also checks this but I've found this warning helpful\n",
    "for lib in libs:\n",
    "    if not os.path.isfile(lib):\n",
    "        print(f\"Cannot find the library {lib}\")\n",
    "\n",
    "mo_name = \"KundurSMIB.SMIB_vref\" # name of Modelica model in the Library.Model format\n",
    "env_entry_point = 'examples:DymSMIBEnv' # Python package location of RL environment\n",
    "\n",
    "v_ref = 1\n",
    "time_step = 1 # time delta in seconds\n",
    "positive_reward = 1\n",
    "negative_reward = -100 # penalize RL agent for is_done\n",
    "log_level = logging.DEBUG\n",
    "\n",
    "# these config values are passed to the model specific environment class\n",
    "# mo_name and libs are passed on to the DymolaBaseEnv class\n",
    "config = {\n",
    "    'mo_name': mo_name,\n",
    "    'libs': libs,\n",
    "    'v_ref': v_ref,\n",
    "    'time_step': time_step,\n",
    "    'positive_reward': positive_reward,\n",
    "    'negative_reward': negative_reward,\n",
    "    'log_level': log_level\n",
    "}\n",
    "\n",
    "# enable the model specific class as an OpenAI gym environment\n",
    "from gym.envs.registration import register\n",
    "env_name = \"MicrogridEnv-v0\"\n",
    "\n",
    "register(\n",
    "    id=env_name,\n",
    "    entry_point=env_entry_point,\n",
    "    kwargs=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a70b103e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing old files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aisling\\.conda\\envs\\myenv\\lib\\site-packages\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "# create the environment. this will run an initial step and must return [True, [...]] or something is broken\n",
    "# TODO: create error handling/warnings if simulations don't work (i.e. returns [False], [...])\n",
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ba0ee00",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing old files...\n",
      "[True, [1.004036435712441, 2.4]]\n",
      "[True, [0.9434768685264913, 2.4]]\n",
      "[True, [0.8764119088027409, 2.4]]\n",
      "[True, [0.8364366439252497, 2.4]]\n",
      "[True, [0.8408108452652046, 2.4]]\n",
      "[True, [0.879102182918432, 2.4]]\n",
      "[True, [0.9147671483845703, 2.4]]\n",
      "[True, [0.9284701799284032, 2.4]]\n",
      "[True, [0.9185757247925377, 2.4]]\n",
      "[True, [0.8953437650988731, 2.4]]\n"
     ]
    }
   ],
   "source": [
    "# this bit is for normalizing the reward later (to improve training), can be safely ignored for now\n",
    "min_reward = np.inf\n",
    "max_reward = -np.inf\n",
    "avg_reward = 0\n",
    "obs = env.reset()\n",
    "\n",
    "# show performance over 10 seconds in a do-nothing case (control voltage set at 1.0 pu)\n",
    "for _ in range(10):\n",
    "    action = [2.4] # control voltage = 2.4 pu\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    if done:\n",
    "        env.reset()\n",
    "    \n",
    "    # a continuation of the reward normalizing piece (can be ignored for now)\n",
    "    avg_reward += 1/30 * reward\n",
    "    if reward < min_reward:\n",
    "        min_reward = reward\n",
    "    if reward > max_reward:\n",
    "        max_reward = reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d6d804c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[True, [1.004036435712441, 2.4]]\n",
      "[True, [0.9434775763840382, 2.4]]\n",
      "[True, [0.8764145590134519, 2.4]]\n",
      "[True, [0.8364397731572241, 2.4]]\n",
      "[True, [0.8408130279189785, 2.4]]\n",
      "[True, [0.8791020200035254, 2.4]]\n",
      "[True, [0.9147662176596749, 2.4]]\n",
      "[True, [0.9284693990952451, 2.4]]\n",
      "[True, [0.9185764052902812, 2.4]]\n",
      "[True, [0.895343659328478, 2.4]]\n"
     ]
    }
   ],
   "source": [
    "dt = 1\n",
    "start = 0\n",
    "stop = 1\n",
    "p_gen = []\n",
    "for i in range(10):\n",
    "    v_ref = 2.4\n",
    "    if i > 0:\n",
    "        env.dymola.importInitialResult('dsres.mat', atTime=start)\n",
    "    res = env.dymola.simulateExtendedModel('KundurSMIB.SMIB_vref', startTime=start, stopTime=stop, \n",
    "                                           initialNames=['v_ref'], initialValues=[v_ref], \n",
    "                                           finalNames=['G1.machine.P', 'v_ref'])\n",
    "    print(res)\n",
    "    start += dt\n",
    "    stop += dt\n",
    "    \n",
    "    p_gen += [res[1][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf07039",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a71901b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "buildlog.txt\n",
      "dsfinal.txt\n",
      "dsin.txt\n",
      "dslog.txt\n",
      "dsmodel.c\n",
      "dsres.mat\n",
      "dymosim.exe\n"
     ]
    }
   ],
   "source": [
    "for file in os.listdir('temp_dir'):\n",
    "    print(file)#os.getcwd()\n",
    "    os.remove(os.path.join(os.getcwd(), 'temp_dir', file))\n",
    "    \n",
    "for file in os.listdir('temp_dir'):\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b91f904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this bit is for normalizing the reward later (to improve training), can be safely ignored for now\n",
    "min_reward = np.inf\n",
    "max_reward = -np.inf\n",
    "avg_reward = 0\n",
    "obs = env.reset()\n",
    "\n",
    "# show performance over 10 seconds in a do-nothing case (control voltage set at 1.0 pu)\n",
    "for _ in range(10):\n",
    "    action = [2.4] # control voltage = 1.0 pu\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    if done:\n",
    "        env.reset()\n",
    "    \n",
    "    # a continuation of the reward normalizing piece (can be ignored for now)\n",
    "    avg_reward += 1/30 * reward\n",
    "    if reward < min_reward:\n",
    "        min_reward = reward\n",
    "    if reward > max_reward:\n",
    "        max_reward = reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d37307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset environment\n",
    "obs = env.reset()\n",
    "\n",
    "# run a randomized agent to verify:\n",
    "#    (1) that the simulation runs when we are controlling and changing an input value\n",
    "#    (2) that the simulation outputs different results than the do-nothing or rule-based controller\n",
    "for _ in range(10):\n",
    "    action = [np.random.uniform(1.0,2.0)]\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    if done:\n",
    "        env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964d4ec6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create a stable-baselines Soft Actor Critic agent\n",
    "model = SAC(MlpPolicy, env, verbose=1, tensorboard_log=\"tensorboard_logs\")\n",
    "\n",
    "# run a short training period to verify that the syntax is ok\n",
    "print(\"Training the model...\")\n",
    "obs = env.reset()\n",
    "model.learn(total_timesteps=20, tb_log_name=\"microgrid\")\n",
    "\n",
    "# run a short test period to verify that the syntax is ok\n",
    "print(\"Testing the model...\")\n",
    "obs = env.reset()\n",
    "rl_reward = 0\n",
    "for _ in range(10):\n",
    "    action, _state = model.predict(obs)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    if done:\n",
    "        env.reset()\n",
    "    rl_reward += reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d72b57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-myenv] *",
   "language": "python",
   "name": "conda-env-.conda-myenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
